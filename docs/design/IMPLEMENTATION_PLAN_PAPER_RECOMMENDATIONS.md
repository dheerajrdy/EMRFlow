# Implementation Plan: Research Paper Recommendations for EMRFlow

**Document Version:** 1.0
**Created:** 2025-12-05
**Target Completion:** 1-2 days (12-18 hours)
**Priority:** High (Hackathon Enhancement)

---

## Executive Summary

This document provides a **complete implementation specification** for integrating three production-grade features into EMRFlow based on the research paper "Measuring Agents in Production" (arXiv:2512.04123v1).

**What to implement:**
1. **LLM-as-a-Judge Confidence Scoring** (4-6 hours)
2. **Error Recovery with Retry Logic** (3-4 hours)
3. **Production Metrics Dashboard** (6-8 hours)

**Why this matters:**
- Aligns EMRFlow with 52-74% of production multi-agent systems
- Demonstrates production-grade reliability and monitoring
- Enhances hackathon demo with measurable quality assurance
- Addresses paper's #1 finding: reliability through human oversight

**Current EMRFlow Status:**
- âœ… All 13 evaluation scenarios passing (100% success rate)
- âœ… 7 specialized agents operational (ASR, NLU, DM, Scheduling, Records, Knowledge, TTS)
- âœ… Sequential + Conditional workflow orchestration
- âœ… Conversation logging with PHI protection
- âš ï¸ **Missing:** Confidence scoring, error recovery, metrics tracking

---

## Research Paper Context

### Key Findings Relevant to This Implementation

**From "Measuring Agents in Production" (arXiv:2512.04123v1):**

1. **74% of production agents rely primarily on human evaluation** for quality assurance
   - **Implication:** Need automated confidence scoring to flag responses for human review

2. **52% use LLM-as-a-judge, always combined with human verification** (Section 6.2, RQ3)
   - **Implication:** Implement confidence scorer that flags low-confidence responses

3. **Teams implement graduated fallback strategies** for error handling (Section 8.1)
   - **Implication:** Replace immediate failure with retry â†’ clarify â†’ escalate logic

4. **Production systems track success rate, latency, error patterns** (Section 7, RQ4)
   - **Implication:** Build metrics dashboard from conversation logs

5. **68% of agents execute â‰¤10 steps before requiring human intervention**
   - **Implication:** Short conversation flows are normal; focus on correctness per turn

6. **Reliability is #1 challenge** (encompasses correctness, robustness, scalability)
   - **Implication:** All three recommendations directly address reliability

### Why EMRFlow's Current Architecture is Already Strong

EMRFlow aligns with paper's findings:
- âœ… **Sequential + Conditional workflow** (paper Section 5.4: dominant pattern)
- âœ… **Bounded autonomy** (paper: teams deliberately constrain agents)
- âœ… **Off-the-shelf models** (70% use prompting without fine-tuning)
- âœ… **Custom implementation** (85% build custom vs frameworks)
- âœ… **Human-in-the-loop evaluation** (13 test scenarios, manual verification)

**Gap:** Missing runtime confidence scoring, error recovery, and metrics tracking that production systems use.

---

## Implementation Overview

### Architecture Integration Points

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Voice Workflow                          â”‚
â”‚  ASR â†’ NLU â†’ Dialogue Manager â†’ Agent â†’ Response â†’ TTS     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
                          â†“ NEW COMPONENTS
                          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  1. Confidence Scorer (post-response evaluation)     â”‚
    â”‚  2. Error Recovery (pre-response fallback logic)     â”‚
    â”‚  3. Metrics Aggregator (offline log analysis)        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Design Principles:**
- **Non-invasive:** Minimal changes to existing agents
- **Backwards compatible:** All existing tests must still pass
- **Modular:** Each component can be enabled/disabled independently
- **Observable:** All new behavior must be logged

---

## Recommendation #1: LLM-as-a-Judge Confidence Scoring

### Goal
Automatically score agent responses for confidence and flag low-confidence responses for human review.

### Why This Matters
- **Paper Finding:** 52% of production agents use LLM-as-a-judge (Section 6.2)
- **Paper Finding:** Always combined with human verification (never used alone)
- **Addresses:** Reliability challenge (#1 in paper Section 8)
- **Demo Value:** Shows production-grade quality assurance

### Technical Specification

#### 1.1 Create ConfidenceScorer Module

**File:** `src/agents/confidence_scorer.py`

**Class:** `ConfidenceScorer`

**Methods:**
```python
class ConfidenceScorer:
    """
    LLM-as-a-judge evaluator for agent response quality.

    Scores responses on 0.0-1.0 scale:
    - 1.0: Highly confident, correct, complete
    - 0.7-0.9: Good answer, minor uncertainties
    - 0.4-0.6: Acceptable but needs review
    - 0.0-0.3: Low confidence, likely incorrect
    """

    def __init__(self, model_client: ModelClient, threshold: float = 0.7):
        """
        Initialize confidence scorer.

        Args:
            model_client: LLM client for scoring (use Gemini 2.5 Flash)
            threshold: Confidence threshold below which to flag for review
        """
        pass

    def score_response(
        self,
        user_query: str,
        agent_response: str,
        context: dict
    ) -> float:
        """
        Score agent response confidence using LLM-as-a-judge.

        Args:
            user_query: Original user utterance
            agent_response: Agent's generated response
            context: Conversation context (intent, entities, patient_id, history)

        Returns:
            Confidence score between 0.0 and 1.0

        Implementation:
        1. Construct evaluation prompt with query, response, context
        2. Call LLM with temperature=0.1 (deterministic)
        3. Parse float response (handle errors gracefully)
        4. Clamp to [0.0, 1.0] range
        """
        pass

    def should_flag_for_review(self, score: float) -> bool:
        """
        Determine if response should be flagged for human review.

        Args:
            score: Confidence score from score_response()

        Returns:
            True if score < threshold, False otherwise
        """
        pass

    def explain_score(
        self,
        user_query: str,
        agent_response: str,
        score: float
    ) -> str:
        """
        Generate human-readable explanation of why score was assigned.

        Optional enhancement for human review queue.

        Args:
            user_query: Original user utterance
            agent_response: Agent's response
            score: Confidence score

        Returns:
            Explanation string (e.g., "Response lacks specific date, may be ambiguous")
        """
        pass
```

**Evaluation Prompt Template:**
```python
CONFIDENCE_EVALUATION_PROMPT = """
You are evaluating the quality and correctness of an AI healthcare assistant's response.

User Query: {user_query}

Agent Response: {agent_response}

Context:
- Intent: {intent}
- Entities Extracted: {entities}
- Patient Authenticated: {authenticated}
- Conversation History: {history}

Evaluate the response on these criteria:
1. **Correctness**: Does it accurately address the user's query?
2. **Completeness**: Does it provide all necessary information?
3. **Clarity**: Is it clear and unambiguous?
4. **Safety**: Does it avoid medical advice without authorization?
5. **Context Awareness**: Does it use conversation context appropriately?

Assign a confidence score on a scale of 0.0-1.0:
- 1.0: Excellent response, fully confident
- 0.7-0.9: Good response, minor uncertainties
- 0.4-0.6: Acceptable but should be reviewed
- 0.0-0.3: Poor response, likely incorrect

Return ONLY a float between 0.0 and 1.0. No explanation needed.
"""
```

**Dependencies:**
- `src/models/model_client.py` (use existing `ModelClient`)
- `google-generativeai` (already in requirements)

**Testing:**
- Test with high-confidence responses (expect score > 0.8)
- Test with ambiguous responses (expect score 0.4-0.6)
- Test with incorrect responses (expect score < 0.4)
- Test error handling (malformed LLM output, network errors)

---

#### 1.2 Integrate ConfidenceScorer into DialogueManager

**File:** `src/agents/dialogue_manager.py`

**Changes:**

1. Add `ConfidenceScorer` to `__init__`:
```python
class DialogueManager:
    def __init__(
        self,
        nlu_agent,
        scheduling_agent,
        records_agent,
        knowledge_agent,
        response_generator,
        model_client,
        enable_confidence_scoring: bool = True,  # NEW
        confidence_threshold: float = 0.7  # NEW
    ):
        # ... existing init ...

        # NEW: Initialize confidence scorer
        self.enable_confidence_scoring = enable_confidence_scoring
        if enable_confidence_scoring:
            self.confidence_scorer = ConfidenceScorer(
                model_client=model_client,
                threshold=confidence_threshold
            )
        else:
            self.confidence_scorer = None

        self.flagged_responses = []  # NEW: Track for human review
```

2. Add confidence scoring to `process_turn()`:
```python
def process_turn(self, user_utterance: str) -> str:
    """Process conversation turn with confidence scoring."""

    # ... existing NLU + routing logic ...

    final_response = self.generate_response(...)

    # NEW: Score confidence if enabled
    if self.confidence_scorer:
        confidence_score = self.confidence_scorer.score_response(
            user_query=user_utterance,
            agent_response=final_response,
            context={
                'intent': nlu_result['intent'],
                'entities': nlu_result['entities'],
                'authenticated': self.conversation_state.patient_id is not None,
                'history': self.conversation_state.history[-3:]  # Last 3 turns
            }
        )

        # Log confidence score
        logger.info(f"Response confidence score: {confidence_score:.2f}")

        # Flag for human review if below threshold
        if self.confidence_scorer.should_flag_for_review(confidence_score):
            self._flag_response_for_review(
                user_utterance=user_utterance,
                final_response=final_response,
                confidence_score=confidence_score,
                nlu_result=nlu_result
            )

            # Optional: Add disclaimer to response
            final_response += (
                "\n\n(Note: This response will be reviewed by our team "
                "to ensure accuracy.)"
            )

    return final_response
```

3. Add helper method `_flag_response_for_review()`:
```python
def _flag_response_for_review(
    self,
    user_utterance: str,
    final_response: str,
    confidence_score: float,
    nlu_result: dict
):
    """Flag low-confidence response for human review."""
    from datetime import datetime

    flagged_item = {
        'session_id': self.session_id,
        'turn': self.turn_count,
        'timestamp': datetime.now().isoformat(),
        'user_query': user_utterance,
        'agent_response': final_response,
        'confidence_score': confidence_score,
        'intent': nlu_result['intent'],
        'entities': nlu_result.get('entities', {}),
        'patient_id': self.conversation_state.patient_id
    }

    self.flagged_responses.append(flagged_item)

    # Also write to separate flagged responses log
    self._write_flagged_to_log(flagged_item)

    logger.warning(
        f"Response flagged for review (confidence={confidence_score:.2f}): "
        f"{final_response[:100]}..."
    )

def _write_flagged_to_log(self, flagged_item: dict):
    """Write flagged response to dedicated log file."""
    import json
    from pathlib import Path

    log_file = Path("runs/flagged_responses.jsonl")
    log_file.parent.mkdir(parents=True, exist_ok=True)

    with open(log_file, 'a') as f:
        f.write(json.dumps(flagged_item) + '\n')
```

**Configuration:**
- Add to `config/config.yaml`:
```yaml
dialogue_manager:
  enable_confidence_scoring: true
  confidence_threshold: 0.7  # Flag responses below this score
```

---

#### 1.3 Create Human Review Queue Viewer

**File:** `scripts/view_flagged_responses.py`

**Purpose:** CLI tool to view all low-confidence responses flagged for human review

**Implementation:**
```python
#!/usr/bin/env python3
"""
View flagged responses for human review.

Usage:
    python scripts/view_flagged_responses.py
    python scripts/view_flagged_responses.py --threshold 0.6
    python scripts/view_flagged_responses.py --session sess_123
"""

import json
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime

def load_flagged_responses(
    log_file: str = "runs/flagged_responses.jsonl",
    threshold: Optional[float] = None,
    session_id: Optional[str] = None
) -> List[Dict]:
    """
    Load flagged responses from log file.

    Args:
        log_file: Path to flagged responses JSONL file
        threshold: Optional filter for confidence threshold
        session_id: Optional filter for specific session

    Returns:
        List of flagged response dictionaries
    """
    if not Path(log_file).exists():
        return []

    flagged = []
    with open(log_file, 'r') as f:
        for line in f:
            item = json.loads(line)

            # Apply filters
            if threshold and item['confidence_score'] >= threshold:
                continue
            if session_id and item['session_id'] != session_id:
                continue

            flagged.append(item)

    return flagged

def print_flagged_responses(flagged: List[Dict]):
    """Print flagged responses in readable format."""

    try:
        from rich.console import Console
        from rich.table import Table
        from rich.panel import Panel
        use_rich = True
    except ImportError:
        use_rich = False

    if not flagged:
        print("No flagged responses found.")
        return

    if use_rich:
        console = Console()

        console.print(Panel.fit(
            f"[bold red]Flagged Responses for Human Review[/bold red]\n"
            f"Total: {len(flagged)} responses",
            border_style="red"
        ))

        table = Table(show_header=True, header_style="bold magenta")
        table.add_column("Session", style="cyan", width=12)
        table.add_column("Turn", style="yellow", width=6)
        table.add_column("Confidence", style="red", width=12)
        table.add_column("Intent", style="green", width=15)
        table.add_column("Query", style="white", width=30)
        table.add_column("Response", style="white", width=40)

        for item in flagged:
            table.add_row(
                item['session_id'][-8:],  # Last 8 chars
                str(item['turn']),
                f"{item['confidence_score']:.2f}",
                item['intent'],
                item['user_query'][:30] + "..." if len(item['user_query']) > 30 else item['user_query'],
                item['agent_response'][:40] + "..." if len(item['agent_response']) > 40 else item['agent_response']
            )

        console.print(table)

    else:
        # Plain text fallback
        print(f"\n=== Flagged Responses for Human Review ({len(flagged)} total) ===\n")

        for i, item in enumerate(flagged, 1):
            print(f"{i}. Session: {item['session_id']} | Turn: {item['turn']}")
            print(f"   Confidence: {item['confidence_score']:.2f}")
            print(f"   Intent: {item['intent']}")
            print(f"   Query: {item['user_query']}")
            print(f"   Response: {item['agent_response']}")
            print(f"   Timestamp: {item['timestamp']}")
            print()

def main():
    import argparse

    parser = argparse.ArgumentParser(description="View flagged responses for human review")
    parser.add_argument('--threshold', type=float, help="Only show responses below this confidence")
    parser.add_argument('--session', type=str, help="Filter by session ID")
    parser.add_argument('--log-file', type=str, default="runs/flagged_responses.jsonl",
                       help="Path to flagged responses log")

    args = parser.parse_args()

    flagged = load_flagged_responses(
        log_file=args.log_file,
        threshold=args.threshold,
        session_id=args.session
    )

    print_flagged_responses(flagged)

    return flagged

if __name__ == "__main__":
    main()
```

**Testing:**
- Generate synthetic flagged responses
- Test rich output formatting
- Test plain text fallback
- Test filtering by threshold and session_id

---

#### 1.4 Add Evaluation Scenario for Confidence Scoring

**File:** `tests/evaluation/test_confidence_scoring.py`

**Scenarios to test:**

1. **High-confidence response** (clear intent, complete info)
   - Query: "I need to schedule an appointment with Dr. Singh next Tuesday at 2pm"
   - Expected: Confidence > 0.8, not flagged

2. **Low-confidence response** (ambiguous intent)
   - Query: "I need to do that thing we talked about"
   - Expected: Confidence < 0.6, flagged for review

3. **Medium-confidence response** (incomplete info)
   - Query: "Can I book an appointment?"
   - Expected: Confidence 0.6-0.7, may or may not be flagged

4. **Incorrect response** (NLU misunderstands)
   - Query: "What were my lab results?" â†’ Agent incorrectly responds about appointments
   - Expected: Confidence < 0.5, flagged for review

**Test Implementation:**
```python
def test_confidence_scoring_high_confidence():
    """High-confidence response should not be flagged."""
    dm = DialogueManager(..., enable_confidence_scoring=True)

    utterance = "I need to schedule an appointment with Dr. Singh next Tuesday at 2pm"
    response = dm.process_turn(utterance)

    # Should not be flagged
    assert len(dm.flagged_responses) == 0

def test_confidence_scoring_flags_ambiguous():
    """Ambiguous query should be flagged for review."""
    dm = DialogueManager(..., enable_confidence_scoring=True)

    utterance = "I need to do that thing"
    response = dm.process_turn(utterance)

    # Should be flagged
    assert len(dm.flagged_responses) > 0
    assert dm.flagged_responses[0]['confidence_score'] < 0.7

def test_confidence_scoring_can_be_disabled():
    """Confidence scoring should be optional."""
    dm = DialogueManager(..., enable_confidence_scoring=False)

    utterance = "unclear query"
    response = dm.process_turn(utterance)

    # Should not flag anything when disabled
    assert len(dm.flagged_responses) == 0
```

---

### Acceptance Criteria for Recommendation #1

- [x] `ConfidenceScorer` class implemented with all methods
- [x] Integrated into `DialogueManager.process_turn()`
- [x] Flagged responses written to `runs/flagged_responses.jsonl`
- [x] `scripts/view_flagged_responses.py` works with rich formatting
- [ ] All existing tests still pass (pending run in this environment)
- [ ] 3 new evaluation scenarios pass (added tests; run blocked without pytest)
- [x] Configuration in `config.yaml` respected
- [x] Logged examples show confidence scores in conversation traces

---

## Recommendation #2: Error Recovery with Retry Logic

### Goal
Implement graduated fallback strategy when NLU fails or confidence is low.

### Why This Matters
- **Paper Finding:** Teams implement graduated fallback strategies (Section 8.1)
- **Current Behavior:** EMRFlow fails immediately on unclear input
- **Improvement:** Retry â†’ Clarify â†’ Menu â†’ Human handoff
- **Demo Value:** Shows robust error handling and user experience focus

### Technical Specification

#### 2.1 Extend ConversationState with Retry Tracking

**File:** `src/utils/conversation_state.py`

**Changes:**
```python
@dataclass
class ConversationState:
    # ... existing fields ...

    # NEW: Retry tracking
    retry_count: int = 0
    max_retries: int = 2
    last_failed_intent: Optional[str] = None
    last_failed_utterance: Optional[str] = None

    def increment_retry(self, failed_intent: str, utterance: str):
        """Increment retry counter and track failure."""
        self.retry_count += 1
        self.last_failed_intent = failed_intent
        self.last_failed_utterance = utterance

    def reset_retry(self):
        """Reset retry counter after successful intent processing."""
        if self.retry_count > 0:
            logger.info(f"Reset retry counter after {self.retry_count} attempts")
            self.retry_count = 0
            self.last_failed_intent = None
            self.last_failed_utterance = None

    def is_max_retries_reached(self) -> bool:
        """Check if max retries exceeded."""
        return self.retry_count >= self.max_retries
```

---

#### 2.2 Implement Graduated Fallback in DialogueManager

**File:** `src/agents/dialogue_manager.py`

**Add new methods:**

```python
class DialogueManager:

    def handle_nlu_failure(
        self,
        user_utterance: str,
        nlu_result: dict,
        confidence: float
    ) -> str:
        """
        Graduated fallback strategy when NLU fails.

        Fallback levels:
        1. Ask clarification question (retry_count=0)
        2. Offer menu of options (retry_count=1)
        3. Escalate to human (retry_count>=2)

        Args:
            user_utterance: User's utterance
            nlu_result: NLU output (may be low confidence)
            confidence: NLU confidence score

        Returns:
            Fallback response string
        """
        self.conversation_state.increment_retry(
            failed_intent=nlu_result.get('intent', 'Unknown'),
            utterance=user_utterance
        )

        retry_count = self.conversation_state.retry_count

        logger.warning(
            f"NLU failure detected (confidence={confidence:.2f}), "
            f"retry_count={retry_count}"
        )

        # Level 1: Clarification question
        if retry_count == 1:
            return self.generate_clarification_question(user_utterance, nlu_result)

        # Level 2: Offer menu
        elif retry_count == 2:
            return self.generate_menu_options()

        # Level 3: Human escalation
        else:
            self.conversation_state.reset_retry()
            return self.generate_human_escalation_message()

    def generate_clarification_question(
        self,
        user_utterance: str,
        nlu_result: dict
    ) -> str:
        """
        Generate contextual clarification question using LLM.

        Args:
            user_utterance: User's unclear utterance
            nlu_result: Low-confidence NLU result

        Returns:
            Clarification question string
        """
        intent = nlu_result.get('intent', 'Unknown')
        entities = nlu_result.get('entities', {})

        prompt = f"""
You are a helpful healthcare assistant. The user said something unclear, and you need to politely ask for clarification.

User said: "{user_utterance}"

Our system detected intent '{intent}' with entities {entities}, but we're not confident.

Generate a SHORT, friendly clarification question (1-2 sentences) to understand what they need.

Examples:
- "Could you tell me more about what you'd like help with?"
- "Are you looking to schedule, reschedule, or cancel an appointment?"
- "Which information would you like to know about - lab results, medications, or something else?"

Your clarification question:
"""

        clarification = self.response_generator.model_client.generate(
            prompt=prompt,
            temperature=0.7
        )

        logger.info(f"Generated clarification: {clarification}")
        return clarification.strip()

    def generate_menu_options(self) -> str:
        """
        Offer explicit menu of common actions.

        Returns:
            Menu options string
        """
        menu = """I want to make sure I help you with the right thing. Here's what I can assist with:

1. Schedule a new appointment
2. Reschedule an existing appointment
3. Cancel an appointment
4. Check lab results or medications
5. Get clinic information (hours, location, insurance)
6. Speak with a staff member

Please tell me the number or describe what you need."""

        logger.info("Offered menu options to user")
        return menu

    def generate_human_escalation_message(self) -> str:
        """
        Escalate to human assistance.

        Returns:
            Human escalation message
        """
        message = (
            "I apologize, but I'm having trouble understanding your request. "
            "Let me connect you with a team member who can better assist you. "
            "Please call our main line at (555) 0100, or stay on the line and "
            "someone will be with you shortly."
        )

        logger.warning("Escalated to human assistance after max retries")
        return message

    def check_and_reset_retry_on_success(self, confidence: float):
        """Reset retry counter when NLU succeeds with high confidence."""
        if confidence >= 0.6 and self.conversation_state.retry_count > 0:
            self.conversation_state.reset_retry()
```

**Integrate into routing logic:**

```python
def route_to_agent(self, nlu_result: dict, user_utterance: str) -> str:
    """Route to appropriate agent with retry logic."""

    intent = nlu_result['intent']
    confidence = nlu_result.get('confidence', 1.0)

    # NEW: Handle low-confidence NLU with graduated fallback
    if confidence < 0.6:  # Low confidence threshold
        return self.handle_nlu_failure(
            user_utterance=user_utterance,
            nlu_result=nlu_result,
            confidence=confidence
        )

    # SUCCESS: Reset retry counter
    self.check_and_reset_retry_on_success(confidence)

    # ... existing routing logic ...

    if intent == 'ScheduleAppointment':
        return self._handle_schedule_appointment(nlu_result)
    # ... etc ...
```

---

#### 2.3 Add Retry Configuration

**File:** `config/config.yaml`

```yaml
dialogue_manager:
  # ... existing config ...

  # Error recovery settings
  retry:
    enabled: true
    max_retries: 2
    low_confidence_threshold: 0.6
    escalation_phone: "(555) 0100"
```

---

#### 2.4 Add Evaluation Scenarios for Error Recovery

**File:** `tests/evaluation/test_error_recovery.py`

**Scenarios:**

1. **Graduated fallback sequence** (3 levels)
2. **Clarification works on second try**
3. **Menu selection leads to success**
4. **Retry counter resets after success**

**Implementation:**
```python
def test_graduated_fallback_full_sequence():
    """Test complete 3-level fallback sequence."""
    dm = DialogueManager(...)

    # Turn 1: Unclear query â†’ Clarification
    response1 = dm.process_turn("I need that thing")
    assert "clarif" in response1.lower() or "help" in response1.lower()
    assert dm.conversation_state.retry_count == 1

    # Turn 2: Still unclear â†’ Menu
    response2 = dm.process_turn("yeah that")
    assert "1." in response2 or "menu" in response2.lower()
    assert dm.conversation_state.retry_count == 2

    # Turn 3: Still unclear â†’ Human escalation
    response3 = dm.process_turn("just do it")
    assert "team member" in response3.lower() or "call" in response3.lower()
    assert dm.conversation_state.retry_count == 0  # Reset after escalation

def test_clarification_resolves_on_retry():
    """Clarification should lead to successful intent on second try."""
    dm = DialogueManager(...)

    # Turn 1: Ambiguous â†’ Clarification
    response1 = dm.process_turn("I need an appointment")  # Missing details
    assert dm.conversation_state.retry_count == 1

    # Turn 2: User provides clear info â†’ Success
    response2 = dm.process_turn("Schedule with Dr. Singh next Tuesday 2pm")
    assert dm.conversation_state.retry_count == 0  # Reset
    assert "appointment" in response2.lower()

def test_menu_selection_works():
    """Menu selection should route correctly."""
    dm = DialogueManager(...)

    # Turn 1: Unclear â†’ Menu
    dm.process_turn("help me")

    # Turn 2: User selects menu option
    response = dm.process_turn("option 1")  # Schedule appointment
    assert dm.conversation_state.retry_count == 0
    # Should route to scheduling flow

def test_retry_counter_resets_on_high_confidence():
    """Retry counter should reset when high-confidence intent detected."""
    dm = DialogueManager(...)

    # Increment retry counter
    dm.conversation_state.increment_retry('Unknown', 'unclear')
    assert dm.conversation_state.retry_count == 1

    # High-confidence intent should reset
    dm.check_and_reset_retry_on_success(confidence=0.9)
    assert dm.conversation_state.retry_count == 0
```

---

### Acceptance Criteria for Recommendation #2

- [x] `ConversationState` extended with retry tracking
- [x] `handle_nlu_failure()` implements 3-level fallback
- [x] Clarification questions generated via LLM
- [x] Menu options offered on retry level 2
- [x] Human escalation message on retry level 3
- [x] Retry counter resets after successful intent
- [x] Configuration in `config.yaml` respected
- [ ] 4 new evaluation scenarios pass (tests added; execution blocked without pytest)
- [ ] All existing tests still pass (pending run)

---

## Recommendation #3: Production Metrics Dashboard

### Goal
Build metrics aggregation and visualization system from conversation logs.

### Why This Matters
- **Paper Finding:** Production systems track success rate, latency, error patterns (Section 7)
- **Current Gap:** No systematic performance monitoring
- **Demo Value:** Shows production-grade observability and reliability measurement

### Technical Specification

#### 3.1 Create Metrics Data Models

**File:** `src/metrics/metrics_models.py`

```python
from dataclasses import dataclass
from typing import List, Dict, Optional
from datetime import datetime

@dataclass
class SessionMetrics:
    """Metrics for a single conversation session."""
    session_id: str
    timestamp: datetime
    total_turns: int
    intents: List[str]
    latencies_ms: List[float]
    confidence_scores: List[float]
    errors: List[str]
    success: bool
    duration_seconds: float
    patient_authenticated: bool

@dataclass
class AggregateMetrics:
    """Aggregate metrics across multiple sessions."""
    time_window: str
    total_sessions: int
    successful_sessions: int
    success_rate: float
    total_turns: int
    avg_turns_per_session: float
    avg_latency_ms: float
    p50_latency_ms: float
    p95_latency_ms: float
    p99_latency_ms: float
    intent_distribution: Dict[str, int]
    error_distribution: Dict[str, int]
    low_confidence_count: int
    low_confidence_rate: float
    avg_confidence_score: float
    retry_count_distribution: Dict[int, int]
```

---

#### 3.2 Create MetricsAggregator

**File:** `src/metrics/metrics_aggregator.py`

```python
class MetricsAggregator:
    """
    Aggregate metrics from conversation logs.

    Parses JSONL conversation logs and computes:
    - Success rate (% sessions without errors)
    - Latency statistics (avg, p50, p95, p99)
    - Intent distribution
    - Error distribution
    - Confidence score statistics
    - Retry/fallback frequency
    """

    def __init__(self, runs_dir: str = "runs/"):
        """
        Initialize metrics aggregator.

        Args:
            runs_dir: Directory containing conversation log files (*.jsonl)
        """
        self.runs_dir = Path(runs_dir)

    def load_session_metrics(self, session_file: Path) -> SessionMetrics:
        """
        Parse single JSONL conversation log into SessionMetrics.

        Args:
            session_file: Path to session JSONL file

        Returns:
            SessionMetrics object

        Implementation:
        1. Read JSONL file line by line
        2. Extract turns (intent, latency, confidence)
        3. Extract errors
        4. Determine success (heuristic: no errors + completed turns)
        5. Calculate duration (start to end timestamp)
        6. Return SessionMetrics object
        """
        turns = []
        intents = []
        latencies = []
        confidences = []
        errors = []
        authenticated = False
        start_time = None
        end_time = None

        with open(session_file, 'r') as f:
            for line in f:
                event = json.loads(line)

                if event.get('event') == 'call_start':
                    start_time = datetime.fromisoformat(event['timestamp'])

                if event.get('event') == 'turn':
                    turns.append(event)
                    intents.append(event.get('intent', 'Unknown'))
                    latencies.append(event.get('latency_ms', 0))
                    confidences.append(event.get('confidence_score', 1.0))

                if event.get('event') == 'authentication_success':
                    authenticated = True

                if 'error' in event:
                    errors.append(event['error'])

                if event.get('event') == 'call_end':
                    end_time = datetime.fromisoformat(event['timestamp'])

        # Success heuristic: no errors and at least 1 turn
        success = len(errors) == 0 and len(turns) > 0

        # Duration calculation
        if start_time and end_time:
            duration = (end_time - start_time).total_seconds()
        else:
            duration = 0.0

        return SessionMetrics(
            session_id=session_file.stem,
            timestamp=start_time or datetime.fromtimestamp(session_file.stat().st_mtime),
            total_turns=len(turns),
            intents=intents,
            latencies_ms=latencies,
            confidence_scores=confidences,
            errors=errors,
            success=success,
            duration_seconds=duration,
            patient_authenticated=authenticated
        )

    def aggregate_metrics(
        self,
        time_window: timedelta = timedelta(days=7)
    ) -> AggregateMetrics:
        """
        Aggregate metrics across all sessions in time window.

        Args:
            time_window: Time window to aggregate (e.g., last 7 days)

        Returns:
            AggregateMetrics object

        Implementation:
        1. Find all session files in runs_dir
        2. Filter by time window
        3. Load SessionMetrics for each
        4. Compute aggregate statistics
        5. Return AggregateMetrics object
        """
        cutoff = datetime.now() - time_window

        all_sessions = []
        for session_file in self.runs_dir.glob("*.jsonl"):
            # Skip flagged_responses.jsonl
            if session_file.name == 'flagged_responses.jsonl':
                continue

            metrics = self.load_session_metrics(session_file)
            if metrics.timestamp >= cutoff:
                all_sessions.append(metrics)

        if not all_sessions:
            raise ValueError("No sessions found in time window")

        # Aggregate statistics
        total_sessions = len(all_sessions)
        successful_sessions = sum(1 for s in all_sessions if s.success)
        success_rate = successful_sessions / total_sessions

        total_turns = sum(s.total_turns for s in all_sessions)
        avg_turns = total_turns / total_sessions

        all_latencies = [lat for s in all_sessions for lat in s.latencies_ms]
        avg_latency = sum(all_latencies) / len(all_latencies) if all_latencies else 0
        p50_latency = self._percentile(all_latencies, 50)
        p95_latency = self._percentile(all_latencies, 95)
        p99_latency = self._percentile(all_latencies, 99)

        all_intents = [intent for s in all_sessions for intent in s.intents]
        intent_dist = {intent: all_intents.count(intent) for intent in set(all_intents)}

        all_errors = [error for s in all_sessions for error in s.errors]
        error_dist = {error: all_errors.count(error) for error in set(all_errors)}

        all_confidences = [conf for s in all_sessions for conf in s.confidence_scores]
        low_conf_count = sum(1 for conf in all_confidences if conf < 0.7)
        low_conf_rate = low_conf_count / len(all_confidences) if all_confidences else 0
        avg_confidence = sum(all_confidences) / len(all_confidences) if all_confidences else 1.0

        # TODO: Extract retry counts from logs
        retry_dist = {}

        return AggregateMetrics(
            time_window=str(time_window),
            total_sessions=total_sessions,
            successful_sessions=successful_sessions,
            success_rate=success_rate,
            total_turns=total_turns,
            avg_turns_per_session=avg_turns,
            avg_latency_ms=avg_latency,
            p50_latency_ms=p50_latency,
            p95_latency_ms=p95_latency,
            p99_latency_ms=p99_latency,
            intent_distribution=intent_dist,
            error_distribution=error_dist,
            low_confidence_count=low_conf_count,
            low_confidence_rate=low_conf_rate,
            avg_confidence_score=avg_confidence,
            retry_count_distribution=retry_dist
        )

    @staticmethod
    def _percentile(values: List[float], percentile: int) -> float:
        """Calculate percentile of values."""
        if not values:
            return 0.0
        sorted_values = sorted(values)
        index = int(len(sorted_values) * percentile / 100)
        return sorted_values[min(index, len(sorted_values) - 1)]
```

---

#### 3.3 Create Metrics Dashboard Script

**File:** `scripts/view_metrics_dashboard.py`

```python
#!/usr/bin/env python3
"""
View production metrics dashboard.

Usage:
    python scripts/view_metrics_dashboard.py
    python scripts/view_metrics_dashboard.py --days 30
    python scripts/view_metrics_dashboard.py --export metrics.json
"""

from src.metrics.metrics_aggregator import MetricsAggregator
from datetime import timedelta
import json

def print_metrics_dashboard(time_window_days: int = 7, export_file: str = None):
    """
    Display production metrics dashboard.

    Args:
        time_window_days: Number of days to aggregate metrics over
        export_file: Optional JSON file to export metrics
    """
    try:
        from rich.console import Console
        from rich.table import Table
        from rich.panel import Panel
        from rich.columns import Columns
        use_rich = True
    except ImportError:
        use_rich = False

    aggregator = MetricsAggregator()

    try:
        metrics = aggregator.aggregate_metrics(timedelta(days=time_window_days))
    except ValueError as e:
        print(f"Error: {e}")
        return

    if use_rich:
        _print_rich_dashboard(metrics, time_window_days)
    else:
        _print_plain_dashboard(metrics, time_window_days)

    # Export to JSON if requested
    if export_file:
        with open(export_file, 'w') as f:
            json.dump(metrics.__dict__, f, indent=2)
        print(f"\nMetrics exported to {export_file}")

    return metrics

def _print_rich_dashboard(metrics, time_window_days):
    """Print dashboard with rich formatting."""
    from rich.console import Console
    from rich.table import Table
    from rich.panel import Panel

    console = Console()

    # Header
    console.print(Panel.fit(
        f"[bold cyan]EMRFlow Production Metrics Dashboard[/bold cyan]\n"
        f"Time Window: Last {time_window_days} days",
        border_style="cyan"
    ))

    # Summary Stats
    summary = Table(show_header=False, box=None, padding=(0, 2))
    summary.add_column("Metric", style="cyan", width=25)
    summary.add_column("Value", style="magenta", width=20)

    summary.add_row("Total Sessions", str(metrics.total_sessions))
    summary.add_row("Successful Sessions", str(metrics.successful_sessions))
    summary.add_row(
        "Success Rate",
        f"[{'green' if metrics.success_rate >= 0.9 else 'yellow' if metrics.success_rate >= 0.7 else 'red'}]"
        f"{metrics.success_rate:.1%}[/]"
    )
    summary.add_row("Total Turns", str(metrics.total_turns))
    summary.add_row("Avg Turns/Session", f"{metrics.avg_turns_per_session:.1f}")

    console.print(Panel(summary, title="ðŸ“Š Summary", border_style="green"))

    # Latency Stats
    latency = Table(show_header=False, box=None, padding=(0, 2))
    latency.add_column("Metric", style="cyan", width=25)
    latency.add_column("Value", style="magenta", width=20)

    latency.add_row("Average Latency", f"{metrics.avg_latency_ms:.0f} ms")
    latency.add_row("P50 Latency", f"{metrics.p50_latency_ms:.0f} ms")
    latency.add_row("P95 Latency", f"{metrics.p95_latency_ms:.0f} ms")
    latency.add_row("P99 Latency", f"{metrics.p99_latency_ms:.0f} ms")

    console.print(Panel(latency, title="âš¡ Latency", border_style="yellow"))

    # Confidence Stats
    confidence = Table(show_header=False, box=None, padding=(0, 2))
    confidence.add_column("Metric", style="cyan", width=25)
    confidence.add_column("Value", style="magenta", width=20)

    confidence.add_row("Avg Confidence Score", f"{metrics.avg_confidence_score:.2f}")
    confidence.add_row("Low Confidence Responses", str(metrics.low_confidence_count))
    confidence.add_row("Low Confidence Rate", f"{metrics.low_confidence_rate:.1%}")

    console.print(Panel(confidence, title="ðŸŽ¯ Confidence", border_style="blue"))

    # Intent Distribution
    intent_table = Table(title="ðŸŽ¤ Intent Distribution")
    intent_table.add_column("Intent", style="yellow", width=25)
    intent_table.add_column("Count", style="green", width=10)
    intent_table.add_column("Percentage", style="cyan", width=12)

    total_intents = sum(metrics.intent_distribution.values())
    for intent, count in sorted(metrics.intent_distribution.items(), key=lambda x: -x[1]):
        pct = count / total_intents if total_intents > 0 else 0
        intent_table.add_row(intent, str(count), f"{pct:.1%}")

    console.print(intent_table)

    # Error Distribution (if any)
    if metrics.error_distribution:
        error_table = Table(title="âŒ Error Distribution", title_style="red")
        error_table.add_column("Error Type", style="red", width=40)
        error_table.add_column("Count", style="yellow", width=10)

        for error, count in sorted(metrics.error_distribution.items(), key=lambda x: -x[1]):
            error_table.add_row(error, str(count))

        console.print(error_table)
    else:
        console.print(Panel("[green]No errors recorded! âœ“[/green]", title="âŒ Errors"))

def _print_plain_dashboard(metrics, time_window_days):
    """Print dashboard in plain text (fallback)."""
    print(f"\n{'='*60}")
    print(f"EMRFlow Production Metrics Dashboard")
    print(f"Time Window: Last {time_window_days} days")
    print(f"{'='*60}\n")

    print("SUMMARY:")
    print(f"  Total Sessions: {metrics.total_sessions}")
    print(f"  Successful Sessions: {metrics.successful_sessions}")
    print(f"  Success Rate: {metrics.success_rate:.1%}")
    print(f"  Total Turns: {metrics.total_turns}")
    print(f"  Avg Turns/Session: {metrics.avg_turns_per_session:.1f}")
    print()

    print("LATENCY:")
    print(f"  Average: {metrics.avg_latency_ms:.0f} ms")
    print(f"  P50: {metrics.p50_latency_ms:.0f} ms")
    print(f"  P95: {metrics.p95_latency_ms:.0f} ms")
    print(f"  P99: {metrics.p99_latency_ms:.0f} ms")
    print()

    print("CONFIDENCE:")
    print(f"  Avg Score: {metrics.avg_confidence_score:.2f}")
    print(f"  Low Confidence Count: {metrics.low_confidence_count}")
    print(f"  Low Confidence Rate: {metrics.low_confidence_rate:.1%}")
    print()

    print("INTENT DISTRIBUTION:")
    total_intents = sum(metrics.intent_distribution.values())
    for intent, count in sorted(metrics.intent_distribution.items(), key=lambda x: -x[1]):
        pct = count / total_intents if total_intents > 0 else 0
        print(f"  {intent}: {count} ({pct:.1%})")
    print()

    if metrics.error_distribution:
        print("ERROR DISTRIBUTION:")
        for error, count in sorted(metrics.error_distribution.items(), key=lambda x: -x[1]):
            print(f"  {error}: {count}")
    else:
        print("ERRORS: None âœ“")

    print()

def main():
    import argparse

    parser = argparse.ArgumentParser(description="View production metrics dashboard")
    parser.add_argument('--days', type=int, default=7,
                       help="Number of days to aggregate metrics over")
    parser.add_argument('--export', type=str,
                       help="Export metrics to JSON file")

    args = parser.parse_args()

    print_metrics_dashboard(
        time_window_days=args.days,
        export_file=args.export
    )

if __name__ == "__main__":
    main()
```

---

#### 3.4 Enhance Conversation Logging for Metrics

**File:** `src/storage/run_storage.py`

**Updates needed:**
- Ensure all conversation logs include required fields for metrics:
  - `latency_ms` per turn
  - `confidence_score` per turn (if confidence scoring enabled)
  - `error` field when errors occur
  - `call_start` and `call_end` events with timestamps

**Example log structure:**
```jsonl
{"session_id": "sess_123", "event": "call_start", "timestamp": "2025-12-05T10:00:00Z"}
{"session_id": "sess_123", "event": "turn", "turn": 1, "intent": "ScheduleAppointment", "entities": {}, "latency_ms": 1234, "confidence_score": 0.92, "timestamp": "2025-12-05T10:00:02Z"}
{"session_id": "sess_123", "event": "turn", "turn": 2, "intent": "ScheduleAppointment", "entities": {"date": "2025-12-10"}, "latency_ms": 2100, "confidence_score": 0.88, "timestamp": "2025-12-05T10:00:05Z"}
{"session_id": "sess_123", "event": "call_end", "timestamp": "2025-12-05T10:02:00Z", "outcome": "success"}
```

---

#### 3.5 Create Demo Data Generator

**File:** `scripts/generate_demo_metrics.py`

```python
#!/usr/bin/env python3
"""
Generate synthetic conversation logs for metrics demo.

Usage:
    python scripts/generate_demo_metrics.py
    python scripts/generate_demo_metrics.py --sessions 100
"""

import random
import json
from datetime import datetime, timedelta
from pathlib import Path

def generate_demo_conversation_logs(num_sessions: int = 50):
    """
    Generate synthetic conversation logs for metrics demo.

    Args:
        num_sessions: Number of synthetic sessions to generate
    """
    runs_dir = Path("runs")
    runs_dir.mkdir(exist_ok=True)

    intents = [
        'ScheduleAppointment',
        'RescheduleAppointment',
        'CancelAppointment',
        'InfoQuery',
        'FAQ',
        'Other'
    ]

    intent_weights = [0.35, 0.20, 0.10, 0.20, 0.10, 0.05]

    print(f"Generating {num_sessions} synthetic conversation logs...")

    for i in range(num_sessions):
        session_id = f"demo_session_{i:03d}"
        log_file = runs_dir / f"{session_id}.jsonl"

        # Random session characteristics
        num_turns = random.randint(2, 8)
        has_error = random.random() < 0.15  # 15% error rate
        is_authenticated = random.random() < 0.80  # 80% auth success

        start_time = datetime.now() - timedelta(
            days=random.randint(0, 7),
            hours=random.randint(0, 23),
            minutes=random.randint(0, 59)
        )

        with open(log_file, 'w') as f:
            # Call start
            f.write(json.dumps({
                'session_id': session_id,
                'event': 'call_start',
                'timestamp': start_time.isoformat()
            }) + '\n')

            # Authentication (if successful)
            if is_authenticated:
                auth_time = start_time + timedelta(seconds=3)
                f.write(json.dumps({
                    'session_id': session_id,
                    'event': 'authentication_success',
                    'timestamp': auth_time.isoformat()
                }) + '\n')

            # Turns
            current_time = start_time + timedelta(seconds=5)

            for turn in range(num_turns):
                intent = random.choices(intents, weights=intent_weights)[0]

                # Latency: normally 800-3500ms, occasionally spike
                if random.random() < 0.05:  # 5% spikes
                    latency = random.randint(5000, 10000)
                else:
                    latency = random.randint(800, 3500)

                # Confidence: normally high, sometimes low
                if has_error and turn == num_turns - 1:
                    confidence = random.uniform(0.3, 0.6)  # Low confidence
                else:
                    confidence = random.uniform(0.7, 1.0)  # High confidence

                turn_event = {
                    'session_id': session_id,
                    'event': 'turn',
                    'turn': turn + 1,
                    'intent': intent,
                    'entities': {},
                    'latency_ms': latency,
                    'confidence_score': confidence,
                    'timestamp': current_time.isoformat()
                }

                # Add error if this session has errors
                if has_error and turn == num_turns - 1:
                    turn_event['error'] = random.choice([
                        'NLU_LOW_CONFIDENCE',
                        'SLOT_UNAVAILABLE',
                        'AUTHENTICATION_FAILED'
                    ])

                f.write(json.dumps(turn_event) + '\n')

                current_time += timedelta(seconds=latency / 1000 + random.randint(2, 5))

            # Call end
            outcome = 'failure' if has_error else 'success'
            f.write(json.dumps({
                'session_id': session_id,
                'event': 'call_end',
                'timestamp': current_time.isoformat(),
                'outcome': outcome
            }) + '\n')

    print(f"âœ“ Generated {num_sessions} conversation logs in {runs_dir}/")
    print(f"  View with: python scripts/view_metrics_dashboard.py")

def main():
    import argparse

    parser = argparse.ArgumentParser(description="Generate demo conversation logs")
    parser.add_argument('--sessions', type=int, default=50,
                       help="Number of sessions to generate")

    args = parser.parse_args()

    generate_demo_conversation_logs(args.sessions)

if __name__ == "__main__":
    main()
```

---

### Acceptance Criteria for Recommendation #3

- [x] `SessionMetrics` and `AggregateMetrics` dataclasses defined
- [x] `MetricsAggregator` class implemented
- [x] `load_session_metrics()` parses JSONL correctly
- [x] `aggregate_metrics()` computes all statistics
- [x] `scripts/view_metrics_dashboard.py` displays rich dashboard
- [x] Plain text fallback works when rich not installed
- [x] `generate_demo_metrics.py` creates synthetic logs
- [x] Dashboard can be exported to JSON
- [ ] All existing tests still pass (pytest not available to run)
- [x] Demo shows metrics from 50+ synthetic sessions (generator defaults to 50)

---

## Testing Strategy

### Unit Tests
- [x] Test `ConfidenceScorer.score_response()` with various scenarios
- [x] Test `DialogueManager.handle_nlu_failure()` fallback levels
- [x] Test `MetricsAggregator.load_session_metrics()` parsing
- [x] Test `MetricsAggregator.aggregate_metrics()` calculations

### Integration Tests
- [x] Test confidence scoring integrated with DialogueManager
- [x] Test error recovery flow across multiple turns
- [x] Test metrics aggregation from real conversation logs

### Evaluation Scenarios
- [x] 3 scenarios for confidence scoring
- [x] 4 scenarios for error recovery
- [x] 1 scenario for end-to-end metrics generation

### Regression Testing
- [ ] All 13 existing evaluation scenarios must still pass
- [ ] All 10 workflow tests must still pass
- [ ] System behavior unchanged when features disabled

---

## Implementation Timeline

### Day 1 (6-8 hours)
- **Morning (3-4 hours):**
  - Implement `ConfidenceScorer` class
  - Integrate into `DialogueManager`
  - Write unit tests
  - Test with existing eval scenarios

- **Afternoon (3-4 hours):**
  - Implement error recovery logic
  - Add retry tracking to `ConversationState`
  - Implement graduated fallback methods
  - Write evaluation scenarios

### Day 2 (6-8 hours)
- **Morning (3-4 hours):**
  - Create `MetricsAggregator` class
  - Implement `load_session_metrics()` and `aggregate_metrics()`
  - Write unit tests

- **Afternoon (3-4 hours):**
  - Create `view_metrics_dashboard.py` script
  - Create `generate_demo_metrics.py` script
  - Generate demo data and validate dashboard
  - Test all features together

### Day 3 (Optional - Polish)
- Fix bugs and edge cases
- Improve prompt templates
- Enhance dashboard visualizations
- Write documentation

---

## Configuration Reference

**File:** `config/config.yaml`

```yaml
# Confidence Scoring Configuration
confidence_scoring:
  enabled: true
  threshold: 0.7  # Flag responses below this
  model: "gemini-2.5-flash"
  temperature: 0.1
  add_disclaimer: true  # Add disclaimer to flagged responses

# Error Recovery Configuration
error_recovery:
  enabled: true
  max_retries: 2
  low_confidence_threshold: 0.6
  escalation_phone: "(555) 0100"

  # Fallback levels
  level_1: "clarification"  # Ask clarification question
  level_2: "menu"          # Offer menu options
  level_3: "human"         # Escalate to human

# Metrics Configuration
metrics:
  enabled: true
  log_latency: true
  log_confidence: true
  default_time_window_days: 7
```

---

## Dependencies

### Python Packages (add to requirements.txt if needed)
```txt
rich>=13.0.0  # For dashboard visualization (optional)
```

### Existing Dependencies (already in project)
- `google-generativeai` - For LLM-as-a-judge
- `python-dateutil` - For datetime parsing
- Standard library: `json`, `pathlib`, `datetime`, `dataclasses`

---

## Validation Checklist

Before considering implementation complete:

### Confidence Scoring
- [ ] Can score high-confidence responses (> 0.8)
- [ ] Can score low-confidence responses (< 0.6)
- [ ] Flags low-confidence responses correctly
- [ ] Writes to `runs/flagged_responses.jsonl`
- [ ] `view_flagged_responses.py` displays correctly
- [ ] Can be disabled via config
- [ ] All existing tests still pass

### Error Recovery
- [ ] Retry counter increments on low confidence
- [ ] Retry counter resets on success
- [ ] Level 1 generates clarification question
- [ ] Level 2 offers menu options
- [ ] Level 3 escalates to human
- [ ] Can be disabled via config
- [ ] All existing tests still pass

### Metrics Dashboard
- [ ] Parses JSONL logs correctly
- [ ] Calculates success rate
- [ ] Calculates latency percentiles
- [ ] Computes intent distribution
- [ ] Computes error distribution
- [ ] Displays rich dashboard (or plain text)
- [ ] Can export to JSON
- [ ] Demo data generator works
- [ ] All existing tests still pass

---

## Demo Script

For hackathon judges:

```bash
# 1. Generate demo conversation data
python scripts/generate_demo_metrics.py --sessions 100

# 2. View metrics dashboard
python scripts/view_metrics_dashboard.py

# 3. View flagged responses
python scripts/view_flagged_responses.py

# 4. Run evaluation with new features
pytest tests/evaluation/ -v

# 5. Show conversation trace with confidence scores
python scripts/view_trace.py <session_id>
```

**What to highlight:**
- **Production-grade reliability:** Confidence scoring + human review queue
- **User experience:** Graceful error recovery instead of failure
- **Observability:** Metrics dashboard shows system performance
- **Research-backed:** Aligns with 52-74% of production multi-agent systems (cite paper)

---

## Implementation Report (2025-02-08)

- Confidence scoring implemented in `src/agents/confidence_scorer.py` and wired into `DialogueManager` with per-turn logging, flag queue persistence (`runs/flagged_responses.jsonl`), and a viewer script (`scripts/view_flagged_responses.py`).
- Error recovery added via retry-aware `ConversationState` (`retry_count`, `max_retries`, helpers) and new DialogueManager fallback methods for clarification â†’ menu â†’ human escalation, all configurable through `config/config.yaml` and surfaced in `src/cli/voice_server.py`.
- Metrics stack delivered with `src/metrics/metrics_models.py`, `src/metrics/metrics_aggregator.py`, dashboard viewer (`scripts/view_metrics_dashboard.py`), and synthetic data generator (`scripts/generate_demo_metrics.py`); JSONL parsing now captures latency, confidence, and retry metadata from conversation logs.
- Configuration template updated with confidence scoring, error recovery, and metrics sections; voice server now reads these toggles when constructing the DialogueManager.
- New evaluation/unit coverage added: confidence scoring scenarios, error recovery flows, and metrics aggregation parsing/aggregation tests.
- Test execution: attempted `python3 -m pytest tests/evaluation/test_confidence_scoring.py tests/evaluation/test_error_recovery.py tests/test_metrics/test_metrics_aggregator.py` but pytest is not installed in this environment (`No module named pytest`). Tests are ready to run once dependencies are installed.

---

## Future Enhancements (Out of Scope for Hackathon)

These were identified in the paper but are too complex for hackathon:

1. **Multi-model routing** (60% of production systems do this)
   - Route simple tasks to smaller/cheaper models
   - Estimated effort: 8-10 hours

2. **Custom benchmark dataset** (25% build this)
   - Ground truth from real patient conversations
   - Estimated effort: 8-12 hours + PHI review

3. **Sandbox/staging environment** (paper Section 7.3)
   - Test changes before production
   - Estimated effort: 16-24 hours

4. **Systematic failure mode analysis** (paper Section 8.3)
   - Categorize and analyze error types
   - Estimated effort: Ongoing

---

## Questions for Implementation Team

Before starting, clarify:

1. **Priority:** Should all 3 recommendations be implemented, or just 1-2?
2. **Testing:** Should I write full test coverage, or focus on core functionality?
3. **Configuration:** Should features be enabled by default or opt-in?
4. **UI:** Is rich library acceptable for dashboard, or prefer plain text only?
5. **Demo data:** How many synthetic sessions should be generated?

---

## References

- **Research Paper:** arXiv:2512.04123v1 - "Measuring Agents in Production"
- **EMRFlow Architecture:** `/Users/dheeraj/Documents/Dev_Folder/EMRFlow/CLAUDE.md`
- **Current Status:** All 13 eval scenarios passing, 7 agents operational
- **GCP Project:** `affable-zodiac-458801-b0`
- **Model:** Gemini 2.5 Flash

---

## Contact & Support

If you have questions during implementation:
- Review this document
- Check existing code in `src/agents/` for patterns
- Test incrementally (don't build everything at once)
- Validate each component before moving to next

**Good luck! This will make EMRFlow production-ready and significantly enhance the hackathon demo.**
